{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the docs for doing inference with the TorchJune model. For the documentation of TorchJune visit todo","title":"Home"},{"location":"#welcome-to-the-docs-for-doing-inference-with-the-torchjune-model","text":"For the documentation of TorchJune visit todo","title":"Welcome to the docs for doing inference with the TorchJune model."},{"location":"emulation/gaussian_process/","text":"Gaussian Process Emulation We can use Gaussian Processes as a powerful regression technique that helps us scan the parameter space in a very efficient way. The code package we use is GPyTorch . The first step we need to do is generate samples for the training. 1. Generating samples To configure the SampleGenerator class, we need to specify which parameters to vary, within which range, the number of samples per parameter, and the total number of parameters. An example of a configuration file can be found in configs/sample_generator.yaml : n_samples: 50 n_samples_per_parameter: 10 parameters_to_vary: infection_networks.networks.household.log_beta: [-2,2] june_configuration_file: \"./configs/june_config.yaml\" save_path: \"./data/samples.pkl\" Which can then be run using the script scripts/generate_samples.py like this: python generate_samples.py configs/sample_generator.yaml The parameter n_samples specifies the number of parameters to sample from the latin hyper-cube, while n_samples_per_parameter specifies how many repetitions of a single parameter to make. The results that are stored correspond to the mean and standard deviation of this set of repetitions. The parameters_to_vary section describes what parameter to range and within which range. Note that only interval ranges are currently supported, since we do the sampling from a latin hyper-cube. It is important that the name of the parameters correspond to the path to the specific parameter in the source code, following PyTorch convention for naming model parameters. All parameters that are not specified will be fixed to the values specified in the june_configuration_file . Finally, the samples will be saved as a pickle file in save_path . The pickle file contains a dictionary with 3 fields: import pickle samples_dictionary = pickle.load(open(\"./data/samples.pkl\", \"rb\")) samples_dictionary.keys() # returns [\"parameters\", \"means\", \"stds\"] The samples dictionary contains the parameters varied and the means and standard deviations obtained for those parameters. We currently only support emulating the number of cases per time-step but this will change. 2. Training the emulator Once we have a samples.pkl file, we can setup the emulators for training. We train two emulators per data point: one emulates the mean of the value of interested and the other one emulates the standard deviation. The goal is for the latter to capture the stochastic of the model correctly. An example of a configuration file for the emulator training can be found in configs/emulator.yaml : title: Emulator example configuration file. device: \"cuda:0\" june_configuration_file: \"/home/arnau/code/torch_june_inference/configs/june_config.yaml\" emulator_configuration: time_stamps: [-1] save_path: \"data/emulator.pkl\" samples_path: \"/home/arnau/code/torch_june_inference/data/samples.pkl\" The device parameter allows us to specify the device to use, either CPU or GPU. The june_configuration_file is the baseline configuration for June, so all non-varying parameters can be fixed. In the emulator_configuration section, we can specify which time_stamps of the time series to emulate, where to save our trained emulator, and the path to the generated samples. The emulator can then be trained doing python scripts/train_emulator.py configs/emulator.yaml Once the emulator is trained, we can load it as import pickle emulator = pickle.load(open(\"./data/emulator.pkl\")) And we can compare the predictions to the samples doing: import gpytorch emulator.set_eval() # put emulator in evaluation mode emulator_means = [] emulator_stds = [] param_range = torch.linspace(-2, 2, 100).to(device) for param in param_range: with torch.no_grad(), gpytorch.settings.fast_pred_var(): pp = param.reshape(1,-1) res = emulator(pp) emulator_means.append(res[\"means\"].item()) emulator_stds.append(res[\"stds\"].item()) emulator_means = np.array(emulator_means) emulator_stds = np.array(emulator_stds)","title":"Gaussian Process Emulation"},{"location":"emulation/gaussian_process/#gaussian-process-emulation","text":"We can use Gaussian Processes as a powerful regression technique that helps us scan the parameter space in a very efficient way. The code package we use is GPyTorch . The first step we need to do is generate samples for the training.","title":"Gaussian Process Emulation"},{"location":"emulation/gaussian_process/#1-generating-samples","text":"To configure the SampleGenerator class, we need to specify which parameters to vary, within which range, the number of samples per parameter, and the total number of parameters. An example of a configuration file can be found in configs/sample_generator.yaml : n_samples: 50 n_samples_per_parameter: 10 parameters_to_vary: infection_networks.networks.household.log_beta: [-2,2] june_configuration_file: \"./configs/june_config.yaml\" save_path: \"./data/samples.pkl\" Which can then be run using the script scripts/generate_samples.py like this: python generate_samples.py configs/sample_generator.yaml The parameter n_samples specifies the number of parameters to sample from the latin hyper-cube, while n_samples_per_parameter specifies how many repetitions of a single parameter to make. The results that are stored correspond to the mean and standard deviation of this set of repetitions. The parameters_to_vary section describes what parameter to range and within which range. Note that only interval ranges are currently supported, since we do the sampling from a latin hyper-cube. It is important that the name of the parameters correspond to the path to the specific parameter in the source code, following PyTorch convention for naming model parameters. All parameters that are not specified will be fixed to the values specified in the june_configuration_file . Finally, the samples will be saved as a pickle file in save_path . The pickle file contains a dictionary with 3 fields: import pickle samples_dictionary = pickle.load(open(\"./data/samples.pkl\", \"rb\")) samples_dictionary.keys() # returns [\"parameters\", \"means\", \"stds\"] The samples dictionary contains the parameters varied and the means and standard deviations obtained for those parameters. We currently only support emulating the number of cases per time-step but this will change.","title":"1. Generating samples"},{"location":"emulation/gaussian_process/#2-training-the-emulator","text":"Once we have a samples.pkl file, we can setup the emulators for training. We train two emulators per data point: one emulates the mean of the value of interested and the other one emulates the standard deviation. The goal is for the latter to capture the stochastic of the model correctly. An example of a configuration file for the emulator training can be found in configs/emulator.yaml : title: Emulator example configuration file. device: \"cuda:0\" june_configuration_file: \"/home/arnau/code/torch_june_inference/configs/june_config.yaml\" emulator_configuration: time_stamps: [-1] save_path: \"data/emulator.pkl\" samples_path: \"/home/arnau/code/torch_june_inference/data/samples.pkl\" The device parameter allows us to specify the device to use, either CPU or GPU. The june_configuration_file is the baseline configuration for June, so all non-varying parameters can be fixed. In the emulator_configuration section, we can specify which time_stamps of the time series to emulate, where to save our trained emulator, and the path to the generated samples. The emulator can then be trained doing python scripts/train_emulator.py configs/emulator.yaml Once the emulator is trained, we can load it as import pickle emulator = pickle.load(open(\"./data/emulator.pkl\")) And we can compare the predictions to the samples doing: import gpytorch emulator.set_eval() # put emulator in evaluation mode emulator_means = [] emulator_stds = [] param_range = torch.linspace(-2, 2, 100).to(device) for param in param_range: with torch.no_grad(), gpytorch.settings.fast_pred_var(): pp = param.reshape(1,-1) res = emulator(pp) emulator_means.append(res[\"means\"].item()) emulator_stds.append(res[\"stds\"].item()) emulator_means = np.array(emulator_means) emulator_stds = np.array(emulator_stds)","title":"2. Training the emulator"},{"location":"inference/gradient_descent/","text":"","title":"Gradient descent"},{"location":"inference/hmc/","text":"Markov Chain Monte Carlo In this section we describe how to configure and perform MCMC inference with TorchJune. We use the Pyro programming language to automate the complicated stuff for us. 1. NUTS An example configuration file can be found in configs/pyro.yaml : title: Pyro example configuration file. device: \"cuda:0\" june_configuration_file: \"./configs/june_config.yaml\" results_path: \"./pyro_tests\" inference_configuration: kernel: type: NUTS max_tree_depth: 10 num_samples: 5000 warmup_steps: 500 likelihood: Normal parameters_to_fit: infection_networks.networks.household.log_beta: prior: dist: Normal loc: 0.3 scale: 0.1 infection_networks.networks.company.log_beta: prior: dist: Normal loc: 0.1 scale: 0.1 infection_networks.networks.school.log_beta: prior: dist: Normal loc: 0.1 scale: 0.1 emulator: use_emulator: true #emulator_config_path: \"./configs/emulator.yaml\" emulator_path: \"./data/emulator.pkl\" data: observable: cases_per_timestep: time_stamps: [-1] error: 0.002 observed_data: \"./june_example/results.csv\" Most of the fields are recognisable from previous sections and they have the same meaning here. The inference_configuration section takes care of the specificity of the MCMC inference engine. In this case we use the NUTS kernel, but we could also use HMC , the max_tree_depth is set to 10. Lower values will lead to much faster inference but it may produce biased results. The warmup_steps denotes the number of burn-in samples to draw, and the num_samples parameter is the number of samples of the MCMC chain. We can also specify the type of likelihood function to use, for now only the Normal distribution is supported. The parameters_to_fit section denotes the parameters we want to fit. Note that if we are using an emulator, it is important that we have as many parameters here as the emulator takes as input, in the same order they were sampled. We can specify the prior for each of them, any distribution supported by Pyro is allowed. We can either do inference using the TorchJune model or a trained emulator. This is configures in the emulator section, where we need to specify the emulator path if we are using one. Finally, the data section configures the data we are fitting the model to. We can have multiple observables, at different time_stamps and with different errors. The errors are considered to be Gaussian. Finally the observed_data field is a path to the data file containing a Dataframe where the columns are the observables. We can then run Pyro by doing python scripts/run_pyro.py configs/pyro.yaml The MCMC chain can be easily plotted doing import matplotlib.pyplot as plt import pandas as pd fig, ax = plt.subplots(1, 2, figsize=(10, 4) ) dfw = pd.read_csv(\"./pyro_tests/pyro_chain_Warmup.csv\") dfs = pd.read_csv(\"./pyro_tests/pyro_chain_Sample.csv\") dfw.plot(ax=ax[0], title=\"Warmup\", legend=False) dfs.plot(ax=ax[1], title=\"Samples\", legend=False) ax[0].legend(loc=\"center left\", bbox_to_anchor=(2.2,0.5)) plt.show() Finally, we can use corner.py to plot our posterior estimates import corner f = corner.corner(dfs.values, labels = labels, smooth=2, truths=true_values, bins=25, show_titles=True) where we see that our estimates agree with the values in the original june_config.yaml file :).","title":"Markov Chain Monte Carlo"},{"location":"inference/hmc/#markov-chain-monte-carlo","text":"In this section we describe how to configure and perform MCMC inference with TorchJune. We use the Pyro programming language to automate the complicated stuff for us.","title":"Markov Chain Monte Carlo"},{"location":"inference/hmc/#1-nuts","text":"An example configuration file can be found in configs/pyro.yaml : title: Pyro example configuration file. device: \"cuda:0\" june_configuration_file: \"./configs/june_config.yaml\" results_path: \"./pyro_tests\" inference_configuration: kernel: type: NUTS max_tree_depth: 10 num_samples: 5000 warmup_steps: 500 likelihood: Normal parameters_to_fit: infection_networks.networks.household.log_beta: prior: dist: Normal loc: 0.3 scale: 0.1 infection_networks.networks.company.log_beta: prior: dist: Normal loc: 0.1 scale: 0.1 infection_networks.networks.school.log_beta: prior: dist: Normal loc: 0.1 scale: 0.1 emulator: use_emulator: true #emulator_config_path: \"./configs/emulator.yaml\" emulator_path: \"./data/emulator.pkl\" data: observable: cases_per_timestep: time_stamps: [-1] error: 0.002 observed_data: \"./june_example/results.csv\" Most of the fields are recognisable from previous sections and they have the same meaning here. The inference_configuration section takes care of the specificity of the MCMC inference engine. In this case we use the NUTS kernel, but we could also use HMC , the max_tree_depth is set to 10. Lower values will lead to much faster inference but it may produce biased results. The warmup_steps denotes the number of burn-in samples to draw, and the num_samples parameter is the number of samples of the MCMC chain. We can also specify the type of likelihood function to use, for now only the Normal distribution is supported. The parameters_to_fit section denotes the parameters we want to fit. Note that if we are using an emulator, it is important that we have as many parameters here as the emulator takes as input, in the same order they were sampled. We can specify the prior for each of them, any distribution supported by Pyro is allowed. We can either do inference using the TorchJune model or a trained emulator. This is configures in the emulator section, where we need to specify the emulator path if we are using one. Finally, the data section configures the data we are fitting the model to. We can have multiple observables, at different time_stamps and with different errors. The errors are considered to be Gaussian. Finally the observed_data field is a path to the data file containing a Dataframe where the columns are the observables. We can then run Pyro by doing python scripts/run_pyro.py configs/pyro.yaml The MCMC chain can be easily plotted doing import matplotlib.pyplot as plt import pandas as pd fig, ax = plt.subplots(1, 2, figsize=(10, 4) ) dfw = pd.read_csv(\"./pyro_tests/pyro_chain_Warmup.csv\") dfs = pd.read_csv(\"./pyro_tests/pyro_chain_Sample.csv\") dfw.plot(ax=ax[0], title=\"Warmup\", legend=False) dfs.plot(ax=ax[1], title=\"Samples\", legend=False) ax[0].legend(loc=\"center left\", bbox_to_anchor=(2.2,0.5)) plt.show() Finally, we can use corner.py to plot our posterior estimates import corner f = corner.corner(dfs.values, labels = labels, smooth=2, truths=true_values, bins=25, show_titles=True) where we see that our estimates agree with the values in the original june_config.yaml file :).","title":"1. NUTS"},{"location":"inference/multinest/","text":"MultiNest MultiNest is a nested sampling algorithm. The main advantages is that it supports multiprocessing, so if we have a few GPUs available it can be a very suitable option for inference. The package we use is PyMultinest . The example configuration file can be found in configs/multinest.yaml : title: MultiNest example configuration file. device: \"cuda:0\" june_configuration_file: \"/home/arnau/code/torch_june_inference/configs/june_config.yaml\" results_path: \"./test_multinest\" parameters_to_fit: infection_networks.networks.household.log_beta: prior: dist: Normal loc: 0.3 scale: 0.1 infection_networks.networks.company.log_beta: prior: dist: Normal loc: 0.1 scale: 0.1 data: observed_data: \"./june_example/results.csv\" observable: cases_per_timestep: time_stamps: [-1] error: 0.002 emulator: use_emulator: false emulator_path: \"./data/emulator.pkl\" inference_configuration: likelihood: Normal Which follows the same conventions as the HMC section. Similarly to the HMC section, we can choose to use an emulator or not. To plot the results, we can do: prefix = \"../test_multinest/multinest\" parameters = [\"household\", \"company\", \"school\"] n_params = len(parameters) likelihood = np.abs(np.loadtxt(prefix + \".txt\")[:,1]) plt.plot(likelihood) plt.show() lp = np.loadtxt(prefix + \".txt\")[:,2:] weights = np.loadtxt(prefix + \".txt\")[:,0] names = [param.split(\"_\")[-1] for param in parameters] f = corner.corner(lp, labels = names, smooth=2, truths=true_values, weights=weights, show_titles=True, bins=30)","title":"MultiNest"},{"location":"inference/multinest/#multinest","text":"MultiNest is a nested sampling algorithm. The main advantages is that it supports multiprocessing, so if we have a few GPUs available it can be a very suitable option for inference. The package we use is PyMultinest . The example configuration file can be found in configs/multinest.yaml : title: MultiNest example configuration file. device: \"cuda:0\" june_configuration_file: \"/home/arnau/code/torch_june_inference/configs/june_config.yaml\" results_path: \"./test_multinest\" parameters_to_fit: infection_networks.networks.household.log_beta: prior: dist: Normal loc: 0.3 scale: 0.1 infection_networks.networks.company.log_beta: prior: dist: Normal loc: 0.1 scale: 0.1 data: observed_data: \"./june_example/results.csv\" observable: cases_per_timestep: time_stamps: [-1] error: 0.002 emulator: use_emulator: false emulator_path: \"./data/emulator.pkl\" inference_configuration: likelihood: Normal Which follows the same conventions as the HMC section. Similarly to the HMC section, we can choose to use an emulator or not. To plot the results, we can do: prefix = \"../test_multinest/multinest\" parameters = [\"household\", \"company\", \"school\"] n_params = len(parameters) likelihood = np.abs(np.loadtxt(prefix + \".txt\")[:,1]) plt.plot(likelihood) plt.show() lp = np.loadtxt(prefix + \".txt\")[:,2:] weights = np.loadtxt(prefix + \".txt\")[:,0] names = [param.split(\"_\")[-1] for param in parameters] f = corner.corner(lp, labels = names, smooth=2, truths=true_values, weights=weights, show_titles=True, bins=30)","title":"MultiNest"},{"location":"inference/svi/","text":"Variational Inference Here we describe how to do Variational Inference with TorchJune. We again use the Pyro programming language to automate the complicated stuff for us. An example configuration file can be found in configs/svi.yaml : title: SVI example configuration file. device: \"cpu\" june_configuration_file: \"./configs/june.yaml\" results_path: \"./svi_results\" inference_configuration: optimizer: type: Adam lr: 0.1 n_steps: 10000 loss: type: Trace_ELBO num_particles: 1 parameters_to_fit: infection_networks.networks.household.log_beta: prior: dist: Normal loc: 0.3 scale: 0.1 infection_networks.networks.company.log_beta: prior: dist: Normal loc: 0.1 scale: 0.1 infection_networks.networks.school.log_beta: prior: dist: Normal loc: 0.1 scale: 0.1 infection_networks.networks.visit.log_beta: prior: dist: Normal loc: 0.7 scale: 0.1 emulator: use_emulator: false #emulator_config_path: \"./configs/emulator.yaml\" emulator_path: \"./data/emulator.pkl\" data: observable: cases_per_timestep: time_stamps: [-1] error: 0.002 observed_data: \"./june_example/results.csv\" The interface is very similar to the HMC case, so most of the fields do not need explaining. The particularities of the SVI are specified under inference_configuration , where we specify the optimizer and the loss function to use. We assume that the posterior distribution over the parameters is a Normal distribution, with parameters $$\\mu$$ and $$\\sigma$$. The values of $$\\mu$$ and $$\\sigma$$ are then found through an optimization problem, where we minimize the ELBO loss: We can then plot the value of $$\\mu$$ and $$\\sigma$$ for each parameter over the training period: And finally plot the posteriors:","title":"Variational Inference"},{"location":"inference/svi/#variational-inference","text":"Here we describe how to do Variational Inference with TorchJune. We again use the Pyro programming language to automate the complicated stuff for us. An example configuration file can be found in configs/svi.yaml : title: SVI example configuration file. device: \"cpu\" june_configuration_file: \"./configs/june.yaml\" results_path: \"./svi_results\" inference_configuration: optimizer: type: Adam lr: 0.1 n_steps: 10000 loss: type: Trace_ELBO num_particles: 1 parameters_to_fit: infection_networks.networks.household.log_beta: prior: dist: Normal loc: 0.3 scale: 0.1 infection_networks.networks.company.log_beta: prior: dist: Normal loc: 0.1 scale: 0.1 infection_networks.networks.school.log_beta: prior: dist: Normal loc: 0.1 scale: 0.1 infection_networks.networks.visit.log_beta: prior: dist: Normal loc: 0.7 scale: 0.1 emulator: use_emulator: false #emulator_config_path: \"./configs/emulator.yaml\" emulator_path: \"./data/emulator.pkl\" data: observable: cases_per_timestep: time_stamps: [-1] error: 0.002 observed_data: \"./june_example/results.csv\" The interface is very similar to the HMC case, so most of the fields do not need explaining. The particularities of the SVI are specified under inference_configuration , where we specify the optimizer and the loss function to use. We assume that the posterior distribution over the parameters is a Normal distribution, with parameters $$\\mu$$ and $$\\sigma$$. The values of $$\\mu$$ and $$\\sigma$$ are then found through an optimization problem, where we minimize the ELBO loss: We can then plot the value of $$\\mu$$ and $$\\sigma$$ for each parameter over the training period: And finally plot the posteriors:","title":"Variational Inference"}]}